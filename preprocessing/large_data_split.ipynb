{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Dataset Split\n",
    "This file creates a smaller dataset from a large dataset.  This is useful for manually labeling a the smaller dataset and training a model to create masks for the larger dataset.  This file creates a new dataset with the `raw` directory. The `raw` directory is populated with random files for each class. These files are then uploaded to LabelBox for manual annotations.\n",
    "\n",
    "##### !!!!IF YOU DONT WANT TO SPLIT THE DATASET\n",
    "Use the `im_to_labelbox.ipynb` or `im_to_labelbox.py` files to upload your entire dataset to LabelBox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, random\n",
    "from labelbox_class import LabelBox, remove_all\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelBox API Key\n",
    "api_key = ''\n",
    "large_dataset = '.\\\\..\\\\data\\\\datasets\\\\birds_dataset'\n",
    "\n",
    "dataset_size = 75    # Number of ims per class\n",
    "\n",
    "\n",
    "# LabelBox Project and Ontology Name\n",
    "project_name = \"Small_Birds\"\n",
    "ontology_name = \"Small_Birds\"\n",
    "\n",
    "# TXT file to be made with project and ontology IDs within it\n",
    "proj_id_txt = \".\\\\..\\\\data\\\\projects\\\\\"+project_name+\".txt\"\n",
    "small_dataset = f'.\\\\..\\\\data\\\\datasets\\\\small_{os.path.basename(large_dataset)}'\n",
    "small_size = 8\n",
    "\n",
    "# Directory to the raw data in the dataset (for uploading)\n",
    "raw_dir = os.path.join(small_dataset, 'raw')\n",
    "print(raw_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_small_dataset(old_dir, new_dir, num_images):\n",
    "    for root, dirs, files in os.walk(os.path.join(old_dir,'raw')):\n",
    "        for dir in dirs:\n",
    "            old_sub_dir = os.path.join(root, dir)\n",
    "            new_sub_dir = old_sub_dir.replace(os.path.join(old_dir,'raw'), os.path.join(new_dir,'raw'))\n",
    "            \n",
    "            os.makedirs(new_sub_dir, exist_ok=True)\n",
    "            images = [f for f in os.listdir(old_sub_dir) if os.path.isfile(os.path.join(old_sub_dir, f))]\n",
    "\n",
    "            selected_images = random.sample(images, num_images)\n",
    "            for image in selected_images:\n",
    "                shutil.copy(os.path.join(old_sub_dir, image), new_sub_dir)\n",
    "    for root, dirs, files in os.walk(old_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                shutil.copy(os.path.join(root, file),os.path.join(new_dir, file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(raw_dir):\n",
    "    os.remove(raw_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_small_dataset(large_dataset,small_dataset, small_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates LabelBox Project, Datasets, and Ontology\n",
    "labels = LabelBox(api_key, raw_dir, project_name,ontology_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds all images that could not be uploaded to LabelBox\n",
    "rm_lst = []\n",
    "for error in tqdm(labels.find_error_sets()):\n",
    "    data_lst = [ os.path.join(raw_dir, error.replace(\"-s \", \"'s \"),x) for x in os.listdir(os.path.join(raw_dir, error.replace(\"-s \", \"'s \")))]\n",
    "    rm_lst.append(labels.labelbox_dataset_lst(error.replace(\"-s \", \"'s \"), data_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes images that could not be uploaded from the dataset \n",
    "for im_lst in rm_lst:\n",
    "    for im in im_lst:\n",
    "        print(im)\n",
    "        os.remove(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates TXT file with project and ontolgy IDs\n",
    "if os.path.exists(proj_id_txt):\n",
    "    valid_path = False\n",
    "    count = 0\n",
    "    while not valid_path:\n",
    "        new_name =  proj_id_txt.split(\".txt\")[0] + str(count) + \".txt\"\n",
    "        if not os.path.exists(new_name):\n",
    "            valid_path = True\n",
    "    proj_id_txt = new_name\n",
    "    \n",
    "with open(proj_id_txt, 'w') as f:\n",
    "    f.write(\"Project_name:\"+project_name+\", Project_id:\"+labels.project.uid+\"\\n\"+\n",
    "            \"Ontology_name:\"+ontology_name+\", Ontology_id:\"+labels.ontology)\n",
    "print(\"Project ID: \",labels.project.uid)\n",
    "print(\"Ontology ID: \", labels.ontology)\n",
    "print(\"File made at: \", proj_id_txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
